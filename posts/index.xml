<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Andrea&#39;s Blog</title>
    <link>https://andreahsu.github.io/posts/</link>
    <description>Recent content in Posts on Andrea&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 Jul 2021 12:59:48 +0800</lastBuildDate><atom:link href="https://andreahsu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Build_dict</title>
      <link>https://andreahsu.github.io/2021/build_dict/</link>
      <pubDate>Mon, 19 Jul 2021 12:47:37 +0800</pubDate>
      
      <guid>https://andreahsu.github.io/2021/build_dict/</guid>
      <description>NLP 建立字典/語料庫 流程 舉例: train_text = [&amp;quot;你喜歡吃蘋果&amp;quot;,&amp;quot;我愛喝可口可樂&amp;quot;,&amp;quot;他喜歡喝飲料&amp;quot;] test_text = [&amp;quot;我喜歡吃奇異果&amp;quot;]
先根據train_text建立字典   1.先使用Tokenizer建立有num_words容量的字典 token = Tokenizer(num_words=num_words) Tokenizer用來對文字中的詞進行統計計數，生成文件詞典，以支援基於詞典位序生成文字的向量表示
  2.使用一系列文件來生成token詞典，texts為list類，每個元素為一個文件 token.fit_on_texts(text)   3.輸出看一下建立的辭典   印出每個字出現的次數
print (token.word_counts) 輸出: OrderedDict([(&amp;#39;喜歡&amp;#39;, 2), (&amp;#39;吃&amp;#39;, 1), (&amp;#39;蘋果&amp;#39;, 1), (&amp;#39;我愛喝&amp;#39;, 1), (&amp;#39;可口&amp;#39;, 1), (&amp;#39;可樂&amp;#39;, 1), (&amp;#39;喝&amp;#39;, 1), (&amp;#39;飲料&amp;#39;, 1)]   印出每個字的index
print (token.word_index) 輸出: {&amp;#39;喜歡&amp;#39;: 1, &amp;#39;吃&amp;#39;: 2, &amp;#39;蘋果&amp;#39;: 3, &amp;#39;我愛喝&amp;#39;: 4, &amp;#39;可口&amp;#39;: 5, &amp;#39;可樂&amp;#39;: 6, &amp;#39;喝&amp;#39;: 7, &amp;#39;飲料&amp;#39;: 8}     4.</description>
    </item>
    
    <item>
      <title>NLP_01</title>
      <link>https://andreahsu.github.io/2021/nlp_01/</link>
      <pubDate>Mon, 05 Jul 2021 11:24:09 +0800</pubDate>
      
      <guid>https://andreahsu.github.io/2021/nlp_01/</guid>
      <description>陳蘊農 word embeddings word2vec 5-1,5-2,5-3   word embedding(詞嵌入)   skip-gram model
Goal: 根據目標字，去學習預測出該字周圍可能出現那些字 用一個nereul network去自己學習minimize target 的objectic function 也就是學習如何用一個vector表達一個字
輸入假設為一萬維(字典裡)，那輸出也為一萬維，則希望透過該模型訓練出，是該字周圍的那些字 的維度數值要盡量大，最後再輸出放softmax變成機率分布
其中300即為每個word vector的維度(該字的300個features)
x是輸入該字的向量(大小:v)，W為隱藏層的權重矩陣(大小:vN，有v個字，每個字有N個feature(即為word embedding，代表該字的位置))，h=xW就是取出唯一相關該字的N個feature，
h為大小為N的向量，在去乘上W&amp;rsquo;的每個column，得出當該字為target word時，對第j個column所代表的context word 有多少關聯
最後用softmax得出機率分布
  word2vec training
  更新W&#39; 要做的事: minimize loss function:
(submit c是 window size，c&amp;lt;v) e越大，e*h所佔的比例越重，影響越深，會調整的(對loss function)比較多 Q:ejc(error term)不就只有1,0兩種數值?
  更新W 要做的事: minimize loss function:
(submit v是該字的大小，c&amp;lt;v)
是否是neighbor的EIj * W&amp;rsquo;ij的權重 * Xj(對Xj這個target word造成的影響)
問題:運算量大，假設有10000字，然後真正是neightbor的字只有10個，那要update剩下的9990個字的weight Ans:限制可以更新weight的字的數量
該文章講得很好 例如: Negative sampling 即用來解決這個問題，對於每個訓練樣本，只更新一部分的權重，而非整個神經網路的權重都被更新。舉例來說，當我們用訓練樣本 (input word: “fox”, output word: “quick”) 來訓練我們的神經網路時，”fox” 和 “quick” 都是用 one-hot 編碼來表示。如果我們詞彙表的大小是 10,000 的話，則在輸出層，我們期望對應 “quick” 這個字詞的神經元節點輸出是 1，其他 9.</description>
    </item>
    
  </channel>
</rss>
