<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Andrea&#39;s Blog</title>
    <link>https://andreahsu.github.io/posts/</link>
    <description>Recent content in Posts on Andrea&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Jul 2021 10:05:43 +0800</lastBuildDate><atom:link href="https://andreahsu.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NLP_01</title>
      <link>https://andreahsu.github.io/2021/nlp_01/</link>
      <pubDate>Mon, 05 Jul 2021 11:24:09 +0800</pubDate>
      
      <guid>https://andreahsu.github.io/2021/nlp_01/</guid>
      <description>陳蘊農 word embeddings word2vec 5-1,5-2,5-3   word embedding(詞嵌入)   skip-gram model
Goal: 根據目標字，去學習預測出該字周圍可能出現那些字 用一個nereul network去自己學習minimize target 的objectic function 也就是學習如何用一個vector表達一個字
輸入假設為一萬維(字典裡)，那輸出也為一萬維，則希望透過該模型訓練出，是該字周圍的那些字 的維度數值要盡量大，最後再輸出放softmax變成機率分布
其中300即為每個word vector的維度(該字的300個features)
x是輸入該字的向量(大小:v)，W為隱藏層的權重矩陣(大小:vN，有v個字，每個字有N個feature(即為word embedding，代表該字的位置))，h=xW就是取出唯一相關該字的N個feature，
h為大小為N的向量，在去乘上W&amp;rsquo;的每個column，得出當該字為target word時，對第j個column所代表的context word 有多少關聯
最後用softmax得出機率分布
  word2vec training
  更新W&#39; 要做的事: minimize loss function:
(submit c是 window size，c&amp;lt;v) e越大，e*h所佔的比例越重，影響越深，會調整的(對loss function)比較多 Q:ejc(error term)不就只有1,0兩種數值?
  更新W 要做的事: minimize loss function:
(submit v是該字的大小，c&amp;lt;v)
是否是neighbor的EIj * W&amp;rsquo;ij的權重 * Xj(對Xj這個target word造成的影響)
問題:運算量大，假設有10000字，然後真正是neightbor的字只有10個，那要update剩下的9990個字的weight Ans:限制可以更新weight的字的數量
該文章講得很好 例如: Negative sampling 即用來解決這個問題，對於每個訓練樣本，只更新一部分的權重，而非整個神經網路的權重都被更新。舉例來說，當我們用訓練樣本 (input word: “fox”, output word: “quick”) 來訓練我們的神經網路時，”fox” 和 “quick” 都是用 one-hot 編碼來表示。如果我們詞彙表的大小是 10,000 的話，則在輸出層，我們期望對應 “quick” 這個字詞的神經元節點輸出是 1，其他 9.</description>
    </item>
    
  </channel>
</rss>
